<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>backpropagation for dorks :: jmg</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="the boring bit" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/backpropagation-math/" />





  
  <link rel="stylesheet" href="//localhost:1313/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terminal.min.e24bf84cafb9e94502324a0c4264d65e9ed1870838db3e47393dfaa83dd5abb4.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">




<link rel="stylesheet" href="//localhost:1313/style.css">


<link rel="shortcut icon" href="//localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="backpropagation for dorks">
<meta property="og:description" content="the boring bit" />
<meta property="og:url" content="//localhost:1313/posts/backpropagation-math/" />
<meta property="og:site_name" content="jmg" />

  <meta property="og:image" content="//localhost:1313/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2026-01-09 22:55:03 &#43;0000 UTC" />










  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>

<script>
  MathJax = {
    tex: {
      displayMath: [['$$', '$$']],
      inlineMath: [['$', '$']]
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>




</head>
<body>


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    &lt;jmg&gt;
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/posts">posts</a></li>
        
      
        
          <li><a href="/tags">tags</a></li>
        
      
        
          <li><a href="https://x.com/flowuninformed">twitter</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/posts" >posts</a></li>
        
      
        
          <li><a href="/tags" >tags</a></li>
        
      
        
          <li><a href="https://x.com/flowuninformed" >twitter</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/backpropagation-math/">backpropagation for dorks</a>
  </h1>
  <div class="post-meta"><time class="post-date">2026-01-09</time><span class="post-author">josh</span><span class="post-reading-time">4 min read (648 words)</span></div>

  
    <span class="post-tags">
      
      #<a href="//localhost:1313/tags/ml/">ml</a>&nbsp;
      
      #<a href="//localhost:1313/tags/math/">math</a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <p>this is a companion post to <a href="/posts/first-post/">neural nets from scratch in rust</a>, covering the mathematical foundations of backpropagation.</p>
<h1 id="the-setup">the setup<a href="#the-setup" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<h2>$ y = f(WX + b) $</h2>
<p>the holy grail formula that underpins neural networks is just a linear transform that you then apply a differentiable nonlinear transformation to.</p>
<p>lets consider a single layer neural network and define our shapes:</p>
<ul>
<li>$X \in \mathbb{R}^{N \times P}$ is our input data, where $N$ is the number of samples and $P$ is the feature space dimension</li>
<li>$W \in \mathbb{R}^{P \times C}$ is our weight matrix, where $C$ is the number of output classes</li>
<li>$b \in \mathbb{R}^{1 \times C}$ is our bias vector</li>
<li>$y \in \mathbb{R}^{N \times C}$ is our output logits</li>
</ul>
<p>the forward pass computes:</p>
$$z = XW + b$$<p>where $z \in \mathbb{R}^{N \times C}$ are the raw <strong>logits</strong> before the activation function.</p>
<p><strong>what are logits?</strong> logits are the unnormalized scores for each class - they&rsquo;re just the raw output of the linear transformation before we turn them into probabilities. they can be any real number (positive, negative, large, small) and don&rsquo;t need to sum to 1 or be bounded. think of them as &ldquo;confidence scores&rdquo; that the network assigns to each class, but they&rsquo;re not yet interpretable as probabilities.</p>
<p>we need logits as an intermediate step because:</p>
<ol>
<li>the linear transformation $XW + b$ produces unbounded real values</li>
<li>we want probabilities (bounded between 0 and 1, summing to 1) for classification</li>
<li>softmax bridges this gap by converting logits to probabilities</li>
</ol>
<p>for a classification task, we&rsquo;ll use the softmax activation to turn these logits into probabilities:</p>
$$y_i = \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}$$<p>this gives us a probability distribution over classes for each sample.</p>
<h2 id="the-loss-function">the loss function<a href="#the-loss-function" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>for multi-class classification, we use the cross-entropy loss. given true labels $t \in \{0, 1, ..., C-1\}^N$ (one label per sample), we first convert them to one-hot vectors $T \in \mathbb{R}^{N \times C}$, where $T_{ij} = 1$ if sample $i$ has label $j$, and $0$ otherwise.</p>
<p>the cross-entropy loss is:</p>
$$L = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C} T_{ij} \log(y_{ij})$$<p>or more compactly:</p>
$$L = -\frac{1}{N}\sum_{i=1}^{N} \log(y_{i,t_i})$$<p>where $t_i$ is the true class label for sample $i$.</p>
<p><strong>why cross-entropy?</strong> cross-entropy comes from information theory and measures the difference between two probability distributions. if $T$ is the true distribution (one-hot encoded labels) and $y$ is our predicted distribution, cross-entropy measures how many bits of information we need on average to encode the true distribution using our predicted distribution.</p>
<p>when our predictions match the true labels perfectly, cross-entropy equals the entropy of the true distribution. when our predictions are wrong, cross-entropy is larger - the difference is called the KL divergence. minimizing cross-entropy is equivalent to minimizing KL divergence between predicted and true distributions, which is exactly what we want.</p>
<h2 id="backpropagation">backpropagation<a href="#backpropagation" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>now for the fun part - computing gradients. we need $\frac{\partial L}{\partial W}$ and $\frac{\partial L}{\partial b}$ to update our parameters.</p>
<p>using the chain rule:</p>
$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial W}$$<p>the gradient of cross-entropy loss with respect to the logits after softmax has a beautiful closed form:</p>
$$\frac{\partial L}{\partial z} = \frac{1}{N}(y - T)$$<p>where $y$ are our softmax predictions and $T$ are the one-hot encoded targets. this is why softmax + cross-entropy is so popular - the gradient simplifies elegantly.</p>
<p>now we need $\frac{\partial z}{\partial W}$. recall that $z = XW + b$, so:</p>
$$\frac{\partial z}{\partial W} = X^T$$<p>putting it together:</p>
$$\frac{\partial L}{\partial W} = \frac{1}{N}X^T(y - T)$$<p>similarly for the bias:</p>
$$\frac{\partial L}{\partial b} = \frac{1}{N}\sum_{i=1}^{N}(y_i - T_i) = \frac{1}{N}\mathbf{1}^T(y - T)$$<p>where $\mathbf{1}$ is a vector of ones with length $N$.</p>
<h2 id="gradient-descent">gradient descent<a href="#gradient-descent" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>with these gradients, we can update our parameters:</p>
$$W \leftarrow W - \eta \frac{\partial L}{\partial W}$$<p>
</p>
$$b \leftarrow b - \eta \frac{\partial L}{\partial b}$$<p>where $\eta$ is the learning rate, a hyperparameter that controls how big our steps are.</p>
<p>repeat this process enough times (forward pass, compute loss, backward pass, update weights) and your network learns to classify!</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h"></span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
    
    
      <a href="//localhost:1313/posts/rust-neural-nets/" class="button inline next">
         [<span class="button__text">neural nets from scratch in rust</span>] &gt;
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2026 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
